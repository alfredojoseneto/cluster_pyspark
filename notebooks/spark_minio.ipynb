{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f25e47a-bbe0-45da-b912-92acf2eb1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pv\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, upper, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614bc82-92d3-484d-901d-e630feb8ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define os nomes do bucket e do objeto a ser criado\n",
    "bucket_name = \"lakehouse\"\n",
    "object_name = \"teste.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190b367-ea68-4dcf-bd2a-6fe30bd7190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minio_client() -> Minio:\n",
    "    client = Minio(\n",
    "        \"minio:9000\",\n",
    "        access_key=\"minioadmin\",\n",
    "        secret_key=\"minioadmin\",\n",
    "        secure=False,\n",
    "    )\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f99f65-edee-4bae-80ff-88fdd62235ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session(app_name: str) -> SparkSession:\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        #.config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.788\")\n",
    "        .config(\"spark.jars\",\"/opt/spark/jars/aws-java-sdk-bundle-1.12.788.jar,/opt/spark/jars/hadoop-aws-3.3.4.jar\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4780e014-e7d0-43d8-8692-7e6436fa4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instancia um cliente e cria um bucket caso ele não exista\n",
    "client = get_minio_client()\n",
    "if bucker_name not in client.list_buckets():\n",
    "    client.make_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12862b7-39c6-4140-8efb-bfafbdb54d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria o dataframe e converte em BytesIO para armazenar no bucket via pyarrow\n",
    "df = pd.DataFrame({\n",
    "    'id':[1, 2, 3, 4],\n",
    "    'value' :[10, 20, 30, 40],\n",
    "    'category': ['blue', 'blue', 'red', 'red']\n",
    "})\n",
    "\n",
    "table = pa.Table.from_pandas(df)\n",
    "parquet_buffer = BytesIO()\n",
    "pq.write_table(table, parquet_buffer)\n",
    "parquet_buffer.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e79551-fbbe-4377-9213-011e657df301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# armazena o objeto no bucket no formato parquet\n",
    "client.put_object(\n",
    "    bucket_name=bucket_name,\n",
    "    object_name=object_name,\n",
    "    data=parquet_buffer,\n",
    "    length=parquet_buffer.getbuffer().nbytes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e809b6f-7cab-4664-9340-a39358d48fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instancia uma sessão spark\n",
    "spark = get_spark_session(\"teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86cc0cd-b3a8-421b-855d-28a893a554ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lê o arquivo do bucket\n",
    "spark_df = spark.read.parquet(f\"s3a://{bucket_name}/{object_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778768e-6f5c-4a99-b48f-bfca437cb363",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b75f05a-ccc3-4c0d-b6d7-26b2574ea0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula a média por categoria\n",
    "spark_df_avg = spark_df.groupBy(\"category\").agg(avg(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5e770-9ad0-4467-be67-7f91341d03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# armazena no bucket\n",
    "spark_df_avg.write.parquet(f\"s3a://{bucket_name}/teste_avg.parquet\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
